<!DOCTYPE html> <html lang="en"> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SB5FP22M58"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SB5FP22M58");</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Dr. Cong Zhang</title> <meta name="author" content="Cong Zhang"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="cong, cong zhang, newcaslte, oxford, linguistics, prosody, phonetics, phonology, speech"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/al-folio/assets/img/icon.jpg"/> <link rel="stylesheet" href="/al-folio/assets/css/main.css"> <link rel="canonical" href="http://localhost:4000/al-folio/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/al-folio/assets/js/theme.js"></script> <script src="/al-folio/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/al-folio/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/blog/">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">research</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/al-folio/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/al-folio/projects/">projects</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/resources/">resources</a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/awards/">awards</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Dr. Cong Zhang </h1> <p class="desc">cong.zhang at newcastle.ac.uk</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/al-folio/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/al-folio/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/al-folio/assets/img/prof_pic-1400.webp"></source> <img src="/al-folio/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Âº†ËÅ™</p> <p>Lecturer in Phonetics and Phonology</p> <p>Newcastle University</p> </div> </div> <div class="clearfix"> <p>Hi! I‚Äôm Cong Zhang ([ts ∞ ä≈ã t É…ë≈ã], each with a high-level tone (tone number 55, [more about my name]). I am a Lecturer in Phonetics and Phonology at the <a href="http://ncl.ac.uk/ecls/" target="_blank" rel="noopener noreferrer">School of Education, Communication and Language Sciences</a>, Newcastle University, UK.</p> <p>My research mainly focuses on aspects of speech prosody (e.g. intonation, lexical tone, rhythm), using a variety of approaches including:</p> <ul> <li>Phonetics and Phonology (Laboratory Phonology)</li> <li>Psycholinguistics</li> <li>Computational linguistics</li> <li>Language Acquisition</li> </ul> <p>I received a DPhil degree from the <a href="http://brainlab.clp.ox.ac.uk/" target="_blank" rel="noopener noreferrer">Language and Brain Lab</a>, University of Oxford. My DPhil thesis, supervised by Professor Aditi Lahiri, was about the intonational tunes in a tonal language ‚Äî Tianjin Mandarin (<a href="https://ora.ox.ac.uk/objects/uuid:3149a35c-e6c2-4f43-a41a-bdc08ebf08f6" target="_blank" rel="noopener noreferrer">More about my DPhil project</a>).</p> <p>I did my Master‚Äôs in Linguistics and Language Acquisition from the <a href="https://www.ncl.ac.uk/elll/" target="_blank" rel="noopener noreferrer">School of English Literature, Language and Linguistics</a>, Newcastle University (UK). There, I worked on a number of projects including child language acquisition, second language acquisition, Mandarin lexical tone perception, etc.</p> <p>For my undergraduate degree, I studied Translation and Interpreting at <a href="https://www.bfsu.edu.cn/" target="_blank" rel="noopener noreferrer">Beijing Foreign Studies University</a> (China). I am therefore also interested in studies about translation and interpreting.</p> <p>Following my DPhil, I worked as a TTS linguist (Linguistics Engineer) in A-Lab at Rokid Inc. One of my major projects was Singing Synthesis (text-to-singing). After this, I came back to academia and worked on the ERC project <a href="https://sprintproject.io/" target="_blank" rel="noopener noreferrer">SPRINT</a> (i.e. Speech Prosody in Interaction: The form and function of intonation in human communication, ERC-ADG-835263 ) and further looked into the aspects of English and Greek intonation in speech production.</p> <div class="cv"> </div> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 21, 2023</th> <td> üìë New paper: Kim, Y., Kogan, V. V., &amp; Zhang, C. (2023). Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics. <em>Applied Linguistics</em>, amad039. <a href="https://academic.oup.com/applij/advance-article/doi/10.1093/applin/amad039/7223350?utm_source=authortollfreelink&amp;utm_campaign=applij&amp;utm_medium=email&amp;guestAccessKey=6ebc0401-70dd-4539-982e-25e6608e3a34" target="_blank" rel="noopener noreferrer">(Free-access link)</a> </td> </tr> <tr> <th scope="row">Mar 21, 2023</th> <td> üèÜ I was awarded a Pioneer Award by the Institute of Social Sciences, Newcastle University. </td> </tr> <tr> <th scope="row">Oct 3, 2022</th> <td> üíº Started my job at Language and Speech Sciences, Newcastle University as a Lecturer (tenured Assistant Professor equivalence) in Phonetics and Phonology. </td> </tr> </table> <center><a href="./news/">[News archive]</a></center> </div> </div> <div class="cv"> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Education</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2018 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">DPhil</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">University of Oxford, UK</h6> <ul class="items"> <li> <span class="item">General Linguistics and Comparative Philology</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2012 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">M.A.(distinction)</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Newcastle Univeristy, UK</h6> <ul class="items"> <li> <span class="item">Linguistics &amp; Language acquisition</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2011 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">B.A. (with Excellent Graduate Award)</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Beijing Foreign studies University, China</h6> <ul class="items"> <li> <span class="item">English Language and Literature (Translation and Interpreting)</span> </li> </ul> </div> </div> </li> </ul> </div> </div> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Employement</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2022- </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">Lecturer in Phonetics and Phonology</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Newcastle University, UK</h6> <ul class="items"> <li> <span class="item">School of Education, Communication and Language Sciences</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2019-2022 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">Postdoctoral Research Associate</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">University of Kent, UK (2019-2020) &amp; Radboud University, Netherlands (2020-2022)</h6> <ul class="items"> <li> <span class="item">Speech Prosody in Interaction -- The form and function of intonation in human communication (ERC-ADG-835263)</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;"> 2018-2019 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4">Text-to-Speech Linguist / Linguistics Engineer</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">Algorithm-Lab, Rokid Inc. (Beijing, China)</h6> </div> </div> </li> </ul> </div> </div> </div> <div class="publications"> <h2>Selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AMPPS</abbr></div> <div id="coretta2023multidimensional" class="col-sm-8"> <div class="title">Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses</div> <div class="author"> Ste Coretta,¬†Joseph V Casillas,¬† ...,¬†<em>Cong Zhang</em>,¬† ...,¬†and¬†Timo B Roettger</div> <div class="periodical"> <em>Advances in Methods and Practices in Psychological Sciences</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1177/25152459231162567" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://journals.sagepub.com/doi/reader/10.1177/25152459231162567/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis which can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling, but also from decisions regarding the quantification of the measured behavior. In the present study, we gave the same speech production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further find little to no evidence that the observed variability can be explained by analysts‚Äô prior beliefs, expertise or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system and calibrate their (un)certainty in their conclusions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">coretta2023multidimensional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Coretta, Ste and Casillas, Joseph V and ... and Zhang, Cong and ... and Roettger, Timo B}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Methods and Practices in Psychological Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">gamification</abbr></div> <div id="kim2023collecting" class="col-sm-8"> <div class="title">Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics</div> <div class="author"> Yoolim Kim,¬†Vita V Kogan,¬†and¬†<em>Cong Zhang</em> </div> <div class="periodical"> <em>Applied Linguistics</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1093/applin/amad039" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://academic.oup.com/applij/advance-article/doi/10.1093/applin/amad039/7223350?utm_source=authortollfreelink&amp;utm_campaign=applij&amp;utm_medium=email&amp;guestAccessKey=6ebc0401-70dd-4539-982e-25e6608e3a34&amp;login=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Gamification of behavioral experiments has been applied successfully to research in a number of disciplines, including linguistics. We believe that these methods have been underutilized in applied linguistics, in particular second-language acquisition research. The incorporation of games and gaming elements (gamification) in behavioral experiments has been shown to mitigate many of the practical constraints characteristic of lab settings, such as limited recruitment or only achieving small-scale data. However, such constraints are no longer an issue with gamified and game-based experiments, and as a result, data collection can occur remotely with greater ease and on a much wider scale, yielding data that are ecologically valid and robust. These methods enable the collection of data that are comparable in quality to the data collected in more traditional settings while engaging far more diverse participants with different language backgrounds that are more representative of the greater population. We highlight three successful applications of using games and gamification with applied linguistic experiments to illustrate the effectiveness of such approaches in a greater effort to invite other applied linguists to do the same.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2023collecting</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Yoolim and Kogan, Vita V and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Collecting Big Data Through Citizen Science: Gamification and Game-based Approaches to Data Collection in Applied Linguistics}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{amad039}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0142-6001}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1093/applin/amad039}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP Findings</abbr></div> <div id="zhu2022bootstrapping" class="col-sm-8"> <div class="title">Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings</div> <div class="author"> Jian Zhu,¬†Zuoyu Tian,¬†Yadong Liu,¬†<em>Cong Zhang</em>,¬†and¬†Chia-wen Lo</div> <div class="periodical"> <em>Findings of Empirical Methods in Natural Language Processing</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.findings-emnlp.81/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://aclanthology.org/2022.findings-emnlp.81.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/spoken_sent_embedding" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5¬†0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2022bootstrapping</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Jian and Tian, Zuoyu and Liu, Yadong and Zhang, Cong and Lo, Chia-wen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Findings of Empirical Methods in Natural Language Processing}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP Findings</abbr></div> <div id="sun2022task" class="col-sm-8"> <div class="title">Task effect on L2 rhythm production by Cantonese learners of Portuguese</div> <div class="author"> Yuqi Sun,¬†and¬†<em>Cong Zhang</em> </div> <div class="periodical"> <em>DELTA: Documenta√ß√£o de Estudos em Ling√º√≠stica Te√≥rica e Aplicada</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1590/1678-460X202258943" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.scielo.br/j/delta/a/QwGKgDkWkvJbNsZ9CrgZNvb/?format=pdf&amp;lang=en" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/spoken_sent_embedding" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This study examines L2 Portuguese speech produced by eight native Cantonese speakers from Macao, China. The aims of this study are to investigate (1) whether the speech rhythm in L2 Portuguese is more source-like (more similar to Cantonese) or more target-like (more similar to Portuguese), and (2) whether L2 speech rhythm differs across three different tasks: a reading task, a retelling task, and an interpreting task. Seven rhythm metrics, i.e., %V, ŒîC, ŒîV, VarcoC, VarcoV, rPVI_C, and nPVI_V, were adopted for comparison and investigation. The results showed that L2 Portuguese rhythm produced by Cantonese speakers differed from L1 Portuguese speakers‚Äô rhythm. R-deletion and vowel epenthesis were the reasons for the variabilities and instabilities of L2 Portuguese production by Cantonese learners, as they affect the duration and the number of vowel intervals and consonantal intervals. Moreover, in Cantonese learners‚Äô L2 Portuguese production, the semi-spontaneous tasks (retelling and interpreting) presented a significant difference from the reading task. The driving force for such a difference was the cognitive load behind the tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sun2022task</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sun, Yuqi and Zhang, Cong}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task effect on L2 rhythm production by Cantonese learners of Portuguese}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0102-4450}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1590/1678-460X202258943}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{DELTA: Documenta√ß√£o de Estudos em Ling√º√≠stica Te√≥rica e Aplicada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Pontif√≠cia Universidade Cat√≥lica de S√£o Paulo - PUC-SP}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{202258943}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="zhu2022byt5-g2p" class="col-sm-8"> <div class="title">ByT5 model for massively multilingual grapheme-to-phoneme conversion</div> <div class="author"> Jian Zhu,¬†<em>Cong Zhang</em>,¬†and¬†David Jurgens</div> <div class="periodical"> <em>In Interspeech 2022</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.03067.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lingjzhu/CharsiuG2P" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu2022byt5-g2p</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ByT5 model for massively multilingual grapheme-to-phoneme conversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Jian and Zhang, Cong and Jurgens, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Interspeech</abbr></div> <div id="zhang2021synchronising" class="col-sm-8"> <div class="title">Synchronising Speech Segments with Musical Beats in Mandarin and English Singing</div> <div class="author"> <em>Cong Zhang</em>,¬†and¬†Jian Zhu</div> <div class="periodical"> <em>In Interspeech 2021</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21i_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/zhang21i_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://osf.io/8m5bj/?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Generating synthesised singing voice with models trained on speech data has many advantages due to the models‚Äô flexibility and controllability. However, since the information about the temporal relationship between segments and beats are lacking in speech training data, the synthesised singing may sound off-beat at times. Therefore, the availability of the information on the temporal relationship between speech segments and music beats is crucial. The current study investigated the segment-beat synchronisation in singing data, with hypotheses formed based on the linguistics theories of P-centre and sonority hierarchy. A Mandarin corpus and an English corpus of professional singing data were manually annotated and analysed. The results showed that the presence of musical beats was more dependent on segment duration than sonority. However, the sonority hierarchy and the P-centre theory were highly related to the location of beats. Mandarin and English demonstrated cross-linguistic variations despite exhibiting common patterns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2021synchronising</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Zhu, Jian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Synchronising Speech Segments with Musical Beats in Mandarin and English Singing}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1199--1203}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2021-1841}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JASA</abbr></div> <div id="zhang2021comparing" class="col-sm-8"> <div class="title">Comparing acoustic analyses of speech data collected remotely</div> <div class="author"> <em>Cong Zhang</em>,¬†Kathleen Jepson,¬†Georg Lohfink,¬†and¬†Amalia Arvaniti</div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pubs.aip.org/asa/jasa/article/149/6/3910/1059288/Comparing-acoustic-analyses-of-speech-data" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8269758/pdf/JASMAN-000149-003910_1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Face-to-face speech data collection has been next to impossible globally due to COVID-19 restrictions. To address this problem, simultaneous recordings of three repetitions of the cardinal vowels were made using a Zoom H6 Handy Recorder with external microphone (henceforth H6) and compared with two alternatives accessible to potential participants at home: the Zoom meeting application (henceforth Zoom) and two lossless mobile phone applications (Awesome Voice Recorder, and Recorder; henceforth Phone). F0 was tracked accurately by all devices; however, for formant analysis (F1, F2, F3) Phone performed better than Zoom, i.e. more similarly to H6, though data extraction method (VoiceSauce, Praat) also resulted in differences. In addition, Zoom recordings exhibited unexpected drops in intensity. The results suggest that lossless format phone recordings present a viable option for at least some phonetic studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2021comparing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Jepson, Kathleen and Lohfink, Georg and Arvaniti, Amalia}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Comparing acoustic analyses of speech data collected remotely}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1121/10.0005132}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0001-4966}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Journal of the Acoustical Society of America}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3910--3916}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Acoustical Society of America}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{149}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">speech prosody</abbr></div> <div id="zhang2020segment-sing" class="col-sm-8"> <div class="title">Segment Duration and Proportion in Mandarin Singing</div> <div class="author"> <em>Cong Zhang</em>,¬†and¬†Xinrong Wang</div> <div class="periodical"> <em>In Proc. Speech Prosody 2020</em> 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/speechprosody_2020/zhang20c_speechprosody.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/al-folio/assets/pdf/pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://osf.io/ead87/?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://osf.io/ybdup?view_only=c87fe156d1874ffba8a16cc363b225af" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Presentation</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020segment-sing</span><span class="p">,</span>
  <span class="na">presentation</span> <span class="p">=</span> <span class="s">{https://osf.io/ybdup?view_only=c87fe156d1874ffba8a16cc363b225af}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong and Wang, Xinrong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Segment Duration and Proportion in Mandarin Singing}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. Speech Prosody 2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{596--600}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/SpeechProsody.2020-122}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICPhS</abbr></div> <div id="Zhang2019a" class="col-sm-8"> <div class="title">Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>In Proceeding of 19th International Congress of Phonetic Sciences</em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://icphs2019.org/icphs2019-fullpapers/pdf/full-paper_930.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Teasing apart lexical prosody and sentence prosody has been one of the most difficult tasks in the study of intonational tunes in tonal languages. Are different prosodic manifestations stacked, or are they an integrated whole? With evidence from production and perception data of the intonational yes/no question tune in Tianjin Mandarin at sentence level, this paper proposes that (1) lexical tonal alterations (a.k.a tone sandhi) are lexical-level prosody and do not belong to sentence-level tune; (2) pitch accents induced by information structure are ‚Äúintra-tune‚Äù features, which are such sentence-level prosody features that do not cause sentence type change. Despite being sentence-level prosody features, they are not a part of the tune for intonational yes/no question.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zhang2019a</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceeding of 19th International Congress of Phonetic Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Stacking and Unstacking Prosodies : The Production and Perception of Sentence Prosody in a Tonal Language}}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{intonation,tianjin mandarin,tone}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Speech Prosody</abbr></div> <div id="Zhang2018" class="col-sm-8"> <div class="title">Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls</div> <div class="author"> <em>Cong Zhang</em> </div> <div class="periodical"> <em>In 9th International Conference on Speech Prosody 2018</em> 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-speech.org/archive/speechprosody_2018/zhang18c_speechprosody.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.isca-speech.org/archive/pdfs/speechprosody_2018/zhang18c_speechprosody.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>This paper examines the chanted call tune in Tianjin Mandarin in order to investigate the possibilities of intonational components, i.e. pitch accents, boundary tones, etc., in a tonal language. Six native Tianjin speakers‚Äô production of disyllabic names and kinship terms were recorded. The speech materials were composed of a set of left-prominent disyllabic names and a set of right-prominent disyllabic names. The results show that there is a L% boundary tone at the end of the intonational phrase, regardless of the lexical tones. Different from the IntQ data, the L% boundary tone is phonetically manifested and overrode the lexical tone contours. A H* pitch accent was found to be associated with the H of each lexical tone. Lengthening was also found in the CC tune. The CC tune in Tianjin Mandarin can be represented as follows: [[H*]sustained]higher register + L%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zhang2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Cong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Chanted Call Tune in Tianjin Mandarin: Disyllabic Calls}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{9th International Conference on Speech Prosody 2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/SpeechProsody.2018-106}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{522--526}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ISCA}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%6F%6E%67.%7A%68%61%6E%67@%6E%65%77%63%61%73%74%6C%65.%61%63.%75%6B" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-2561-2113" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=J9ofRc0AAAAJ&amp;hl" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/congzhang365" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://twitter.com/congprosody" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="/al-folio/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Cong Zhang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/al-folio/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js"></script> <script defer src="/al-folio/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>